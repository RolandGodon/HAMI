coucou
# HAMI : "Human Assited Molecular Identification" Framework
**A bioninformatics pipeline for processing Frogs output data, in accordance with the HAMI framework**

- **Please cite:** [![doi](https://img.shields.io/static/v1?label=doi&message=DOI)](DOI)
<br />

**The best of two worlds: toward large-scale monitoring of biodiversity combining COI metabarcoding and optimized parataxonomic validation**
<br />
*Penel, B., Meynard, C.N., Benoit, L., Bourdonn√©, A., Clamens, A.-L., Soldati, L., Migeon, A., Chapuis, M.P., Kergoat, G.J.  and  Haran, J.* <br />
*Ecography, XXXXX, XXXXX 2024*<br />
<br />

## Prerequisites

The role of HAMI snakemake pipeline is to process the output of the FROGS pipeline as part of the HAMI framework. Therefore, to run HAMI pipeline, an abundance data file and a multi-affiliations data file from FROGS are requiered. 
<br />
<br />
Note that to run HAMI pipeline in the context of HAMI framework, it is also necessary that your samples can be distinguished based on their names. Please use alphabetic prefix for this, e.g : 
Metabarcoding samples = META0001 while Barcoding samples = BAR0001 and control samples = NC(P/p | I/i | E/e)00001.
<br />
<br />
Duplicates of metabarcoding samples and control samples are also required. Discrimination between duplicates will be done using a suffix : e.g META0001-A / META0001-B. 
The information to discriminate samples is therefore implemented in the config file (see below).

## Description  

### Tree structure :

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <ul>
        <li><strong>HAMI</strong>
            <ul>
                <li><strong>DATA</strong>  
                    <ul>
                        <li>test_data 
                            <ul>
                                <li>[testCOI_abundance_raw.tsv].tabular</li>
                                <li>[testCOI_multi-affiliations.tsv].tabular</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>HAMI_PIPELINE</strong> 
                    <ul>
                        <li>config.yaml</li>  
                        <li>HAMI_environment.yaml</li>  
                        <li>HAMI_PIPELINE.smk</li>   
                    </ul>
                </li>
                <li><strong>SCRIPTS</strong> 
                    <ul>
                        <li>clean_frogs.R</li>  
                        <li>filter_frogs.R</li>  
                        <li>pseudogene_and_redudancy.py</li>  
                        <li>Separate_data.py</li>  
                        <li>BARCODE_MAKER.py</li> 
                        <li>BARCODE_pseudogene_filter.py</li> 
                    </ul>
                </li>
              <li>ReadME.md</li>
            </ul>
        </li>
    </ul>
</body>
</html>

5 directories, 12 files

### Pipeline description :                   

Snakemake is a workflow management system written in Python. It facilitates the creation and execution of complex data analysis pipelines, particularly in bioinformatics and computational biology. It is linked to a config.yaml file and an environment.yaml file. Please refer to  the Snakemake tutorial for futher explications: [Snakemake tutorials](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html)

Here HAMI_PIPELINE.smk is a Snakemake workflow that is linked to HAMI framework. It is linked to a configuration file (config.yaml) and an environment file (HAMI_environment.yaml).
Please check these files to understand how it works.


The role of this snakemake pipeline is to process the sequencing data at the output of the FROGS pipeline as part of the HAMI framework.
To do this, there are five successive rules:

- All : It represents the final outputs that need to be generated by the workflow. It triggers the execution of the whole workflow.

- Clean_and_Chimeres : R process to clean the taxonomic affiliation names associated which each FROGS OTU and removing chimeric sequences using the dada2 packages (Callahan et al., 2016 - Nature Methods).

- Separate_BARCODING_METABARCODING_data : Python process to disctinguish metabarcoding samples and barcoding samples.

- Filter_METABARCODING_DATA: R process to filter data and remove noise and contamination (3 filtering steps described in Galan et al., 2016 - MSystems and automated in Chapuis et al., 2023 - Molecular Ecology Resources)

- Pseudogene_Filter_and_reduce_redundancy : Python process to remove pseudogene sequence and reduce data redundancy associated with intraspecific diversity

Each rule calls a specific R or Python script stored in SCRIPTS directory. There are two additional scripts available. They allow to remove pseudogene reads from barcoding samples and generate a full length COI barcode according to the optional step of the HAMI framework (not implemented in the HAMI pipeline).

## Outputs :

### Tree structures :
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <ul>
        <li>test_data
            <ul>
                <li><strong>BARCODE</strong>
                    <ul>
                        <li> <strong>abundance_raw_data_BARCODING.tsv </strong></li>
                    </ul>
                </li>
                <li><strong>Chimeras</strong>
                    <ul>
                        <li><em> 2 files</em></li>
                    </ul>
                </li>
                <li><strong>METABARCODING</strong>
                    <ul>
                        <li>final_files</li>
                             <ul>
                                <li><strong>final_abundance_file.tsv</strong></li>
                                <li><strong>cluster_seq_keep.txt</strong></li>
                                <li><strong>cluster_merged.txt</strong></li>
                                <li><strong>summary.txt</strong></li>
                             </ul>
                        <li>intermediary_step</li>
                            <ul>   
                                 <li>pseudogene</li>
                                        <ul>
                                            <li><em> 2 files</em></li>
                                        </ul>
                                 <li><em>4 files</em></li>
                            </ul>
                        <li>Rproducts</li>
                             <ul>
                                 <li><em>13 files</em></li>
                             </ul>
                        <li><em>4 files</em></li>
                    </ul>
                </li>
                <li>[testCOI_abundance_raw.tsv].tabular</li>
                <li>[testCOI_multi-affiliations.tsv].tabular</li>
            </ul>
        </li>
    </ul>
</body>
</html>

8 directories, 32 files


### Final outputs :

According to the metabarcode data, 3 final outputs are produced and stored in DATA/our_data/METABARCODING/final_files repository.

- final_abundance_file.tsv : This is an occurrence table broadly similar to that produced by FROGS, but in which names have been revised and clusters with similar taxonomic affiliation and percent identity have been merged. Each taxonomic affiliation in the occurrence table is associated with the number of clusters that have been merged and the range of variance in percentage of identity corresponding to the merged clusters. The "multi" variable is used to distinguish OTUs with taxonomic affiliations associated with the term "Multi-affiliation" or identity percentages associated with the term "multi-identity". See FROGS documentation for the meaning of these terms.  

- cluster_merged.txt : This is a file that records the name of each cluster that have been merged together in the occurence_file.tsv file.

- cluster_seq_keep.txt : This is a fasta file in which all the DNA sequences that have passed all the filters applied during the HAMI pipeline are recorded and labelled with their taxonomic affiliation and cluster number.

- summary.txt : It is a summary file that gave general information about how much taxonomic affiliation, cluster and number of reads were kept in final_abundance_file and how many were deleted during pseudogene curation step.

According to barcode data, 1 final outputs is produced and saved in DATA/our_data/BARCODE repository.

- abundance_raw_data_BARCODING.tsv : This is an occurrence table similar to that is produced by FROGS, but with revised names and without chimera sequences. Future implementations of this pipeline to process barcode samples will be implemented shortly. This will allow pseudogenes to be processed in a similar way to metabarcoding data, as well as automatic barcode assembly where possible. 




  <br />

## Installation
### Requirements

The following packages must be installed on your system before installation::

* Python >=3.0
* Miniconda3. Please choose the installer corresponding to your OS: [Miniconda dowloads](https://docs.conda.io/en/latest/miniconda.html)
* GIT


Below are debian commands to rapidly install them:
```
sudo apt-get install git
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod u+x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh
```
When installation asks if you want to run conda init, answer yes.
  <br />
After installation ends, reload bash
```
bash 
```

### Installation
First of all, create a directory that corresponds to your project directory.

Download HAMI:
``` bash
git clone --recursive https://github.com/AUTHOR_REPOSITORY/HAMI.git
cd HAMI
```
When the git clone is finished, you should find a tree structure similar to the one shown at the top of this file..
To ensure the functionality of the HAMI pipeline, no changes should be made to the tree structure.
Simply import the data that you wish to process in relation to your project into the /HAMI/DATA/ directory.
As a reminder, this pipeline processes the output data from FROGS pipeline. 


## Setup your environment

A Snakemake environment file is a file that specifies the software dependencies required to run a Snakemake workflow. It typically contains a list of software packages and their versionsthat are required to execute the various steps or rules defined in the workflow. The environment file ensures reproducibility by providing a consistent software environment for running the workflow across different computing environments. Please use this environment file to create a conda environment adapted to run the HAMI framework  (HAMI_PIPELINE.smk).
<br />
<br />
Do it as follows: 
``` bash
conda env create -f HAMI_PIPELINE/HAMI_environment.yaml  --name HAMI_environment
``` 
<br />

## Activate your environment

Before running HAMI pipeline, the previously created environement need to be activate as follows:

``` bash
conda activate HAMI_environment
```
To deactivate your environment use the following line code : 

``` bash
conda deactivate 
```
Don't forget to reactivate your conda environment each time you re-use the pipeline. 

<br />

## Setup your config file

Once the conda environment is set up, it is time to adapt the config file to your project's needs.

A Snakemake configuration file (config.yaml) is a file written in Python that contains configuration settings for a Snakemake workflow. The configuration file allows users to customise various aspects of the workflow, such as file paths and other workflow-specific settings.

Here is the list of specific settings required to run the HAMI pipeline:

- Project name
- Name of the DNA fragment
- Number of treads used to execute some internal process
- Path to your project directory , as well as data directory and script directory
- Name of the first two inputs used in HAMI pipeline (i.e abundance and multi-affiliation files from FROGS)

- Target taxonomic group
- DNA length
- Reading frame of the DNA (Check it before running the pipeline)
- List of the codon stops associated to your type of DNA and organisms
- Arbitrary threshold  to discriminate intra-interspecific genetic distance (typically 97% for animals according to Hebert et al 2003 - Proceedings of the Royal Society of London)

- Samples prefix for metabarcoding samples, barcoding (if implemented), and negative control
- Samples suffix for discriminating duplicate


## Test and run your pipeline

To launch the pipeline from a terminal, go to the HAMI directory.

It is strongly recommended to test if your configuration is valid and matches the analyses you intended. To do so, launch a dry run of the pipeline using the command:

``` bash
snakemake --snakefile HAMI_PIPELINE/HAMI_PIPELINE.smk -np
```
You can start the pipeline if no error is thrown:

``` bash
snakemake --snakefile HAMI_PIPELINE/HAMI_PIPELINE.smk --cores [#cores] all
```
(note that the first launch is quite slow due to R packages installation from BioConductor)

## Contact
*Benoit Penel - PhD student from CBGP laboratory, Montpellier, France.*
penelbenoit@gamil.com

## Licence
HAMI is availale under the Creative Commons Zero v1.0 Universal license
